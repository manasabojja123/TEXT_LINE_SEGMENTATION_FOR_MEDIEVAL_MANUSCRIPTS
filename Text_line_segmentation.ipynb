{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZaZa02y4adr",
        "outputId": "303f2d6f-3ff0-45d0-d5d4-35e07ba8011d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/seam1\n"
          ]
        }
      ],
      "source": [
        "#@title Mount the Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "import sys\n",
        "sys.path.insert(0, '/content/drive/MyDrive/seam1')\n",
        "%cd /content/drive/MyDrive/seam1/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6q6Xj9O2FTXi"
      },
      "outputs": [],
      "source": [
        "#@title Load Image\n",
        "import errno\n",
        "\n",
        "import cv2\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def load_image(path):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), path)   # if the file is not found in the path, raise an exception\n",
        "\n",
        "    if os.path.splitext(path)[1] == '.tif':                #special processing for tif files -- need to analyse\n",
        "        img = np.asarray(Image.open(path), dtype=int)\n",
        "        img[np.where(img == 0)] = 8\n",
        "        img[np.where(img == 1)] = 0\n",
        "        img = np.stack((img,)*3, axis=-1)\n",
        "        img[:, :, 1] = 0\n",
        "        img[:, :, 2] = 0\n",
        "    else:\n",
        "        img = cv2.imread(path)\n",
        "\n",
        "    if img is None:\n",
        "        raise Exception(\"Image is empty or corrupted\", path)\n",
        "\n",
        "    return img\n",
        "\n",
        "def prepare_image(img, testing, cropping=True, vertical=False):\n",
        "    # -------------------------------\n",
        "    start = time.time()\n",
        "    # -------------------------------\n",
        "\n",
        "    if testing:                                    #if testing is true\n",
        "        img[:, :, 0] = 0              #0,2 making as 0\n",
        "        img[:, :, 2] = 0\n",
        "        locations = np.where(img == 127)       # find the locations which is 127\n",
        "        img[:, :, 1] = 0                       #make 1 also 0\n",
        "        img[locations[0], locations[1]] = 255       # in the locations of where 127 is present do 255\n",
        "        if cropping:                                 #if cropping required do other things --- not requries\n",
        "            locs = np.array(np.where(img == 255))[0:2, ]\n",
        "            img = img[np.min(locs[0, :]):np.max(locs[0, :]), np.min(locs[1, :]):np.max(locs[1, :])]\n",
        "\n",
        "    else:\n",
        "        ##################################################################\n",
        "        # Protect against malformed inputs\n",
        "\n",
        "        # Green channel should be empty\n",
        "        assert len(np.unique(img[:, :, 1])) == 1\n",
        "        assert np.unique(img[:, :, 1])[0] == 0\n",
        "\n",
        "        # Red channel should have at most two values: 0 and 128 for boundaries\n",
        "        assert len(np.unique(img[:, :, 2])) <= 2\n",
        "        assert np.unique(img[:, :, 2])[0] == 0\n",
        "        if len(np.unique(img[:, :, 2])) > 1:\n",
        "            assert np.unique(img[:, :, 2])[1] == 128\n",
        "\n",
        "        ##################################################################\n",
        "        # Prepare the image\n",
        "\n",
        "        # Find and remove boundaries: this is necessary as they are marked with 8 in the blue channel as well\n",
        "        locations = np.where(img == 128)\n",
        "        img[locations[0], locations[1]] = 0\n",
        "        # Find regular text and text + decoration\n",
        "        locations_text = np.where(img == 8)\n",
        "        locations_text_decoration = np.where(img == 12)\n",
        "        # Wipe the image\n",
        "        locations_text = np.where(img == 8)\n",
        "        locations_text_decoration = np.where(img == 12)\n",
        "        # Set the text to be white\n",
        "        img[locations_text[0], locations_text[1]] = 255\n",
        "        img[locations_text_decoration[0], locations_text_decoration[1]] = 255\n",
        "\n",
        "    # Rotate 90 degrees to the left the image (for vertical scripts such as Chinese)\n",
        "    if vertical:\n",
        "        img = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "\n",
        "    # -------------------------------\n",
        "    stop = time.time()\n",
        "    logging.info(\"finished after: {diff} s\".format(diff=stop - start))\n",
        "    # -------------------------------\n",
        "\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "afzzsYILFVc6"
      },
      "outputs": [],
      "source": [
        "#@title Preprocessing\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "from skimage import measure\n",
        "\n",
        "import matplotlib\n",
        "\n",
        "#from src.line_segmentation.utils.util import save_img\n",
        "\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def preprocess(image, small_component_ratio):\n",
        "    # -------------------------------\n",
        "    start = time.time()\n",
        "    # -------------------------------\n",
        "\n",
        "    # find the text area and wipe the rest\n",
        "    image = wipe_outside_textarea(image)\n",
        "    save_img(image, path=os.path.join('./output', 'after_wipe.png'), show=False)\n",
        "    # Remove components which are too small in terms of area\n",
        "    image = remove_small_components(image, small_component_ratio)\n",
        "    save_img(image, path=os.path.join('./output', 'after_removesmall.png'), show=False)\n",
        "\n",
        "    # Remove components which are too big in terms of area -> after removing the small ones!\n",
        "    image = remove_big_components(image)\n",
        "    save_img(image, path=os.path.join('./output', 'after_removebig.png'), show=False)\n",
        "\n",
        "    image[image > 255] = 255\n",
        "\n",
        "    # -------------------------------\n",
        "    stop = time.time()\n",
        "    logging.info(\"finished after: {diff} s\".format(diff=stop - start))\n",
        "    # -------------------------------\n",
        "    return image\n",
        "\n",
        "\n",
        "def wipe_outside_textarea(image):\n",
        "\n",
        "    # Save a copy of the original image\n",
        "    ORIGINAL = image\n",
        "\n",
        "    # Get only green channel\n",
        "    image = image[:, :, 1]\n",
        "\n",
        "    # SMOOTH IMAGE ######################################################################\n",
        "    filter_size_H = 64\n",
        "    filter_size_V = 192\n",
        "    kernel = np.ones((filter_size_V, filter_size_H)) / filter_size_H\n",
        "    # Apply averaging filter\n",
        "    image = cv2.filter2D(image, -1, kernel)\n",
        "    #SMOOTH_IMAGE = image\n",
        "    # Draw a vertical line in the middle of the image to prevent 2 paragraphs to be split\n",
        "    image[5:-5, int(image.shape[1] / 2) - 5:int(image.shape[1] / 2) + 5] = 255\n",
        "    save_img(image, path=os.path.join('./output', 'smoothed_image_1.png'), show=False)\n",
        "    # GET BIGGEST COMPONENT #############################################################\n",
        "    # Get contour points of the binary polygon image\n",
        "    tmp = np.ones((image.shape[0], image.shape[1], 3), dtype=np.uint8)\n",
        "    cc = measure.find_contours(image, 200, fully_connected='high')[0]\n",
        "    # Swap the columns of cc as the coordinate are reversed\n",
        "    cc[:, 0], cc[:, 1] = cc[:, 1], cc[:, 0].copy()\n",
        "    # Cast to int to make, once again, cv2.fillPoly() happy\n",
        "    cc = [cc.astype(np.int32, copy=False)]\n",
        "    cv2.fillPoly(tmp, cc, (255, 255, 255))\n",
        "    # DEBUG\n",
        "    save_img(tmp, path=os.path.join('./output', 'smoothed_image.png'), show=False)\n",
        "\n",
        "    # WIPE EVERYTHING OUTSIDE THIS AREA #################################################\n",
        "    # Use 'tmp' as mask on the original image. Pixel with value '0' are text.\n",
        "    tmp = tmp - ORIGINAL\n",
        "    # Prepare image in RBG format s.t. we can use the coordinates systems of tmp\n",
        "    image = np.stack((image,) * 3, axis=-1)\n",
        "    # Wipe the pixels which are not selected by the mask\n",
        "    image[np.where(tmp != 0)] = 0\n",
        "    # DEBUG\n",
        "    save_img(image, path=os.path.join('./output', 'filtered_image.png'), show=False)\n",
        "\n",
        "    \"\"\"\n",
        "    # FILTER WITH VERTICAL PROJECTION PROFILE ###########################################\n",
        "    # Compute projection profile\n",
        "    ver = np.sum(SMOOTH_IMAGE, axis=0)\n",
        "    # Get all values above average\n",
        "    ver_indexes = np.where(ver > np.mean(ver))\n",
        "    # Find the first and last of them\n",
        "    left = np.min(ver_indexes)\n",
        "    right = np.max(ver_indexes)\n",
        "\n",
        "    # Wipe the image on left/right sides\n",
        "    image[:, 0:left] = 0\n",
        "    image[:, right:] = 0\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(ver)\n",
        "    plt.axhline(y=np.mean(ver), color='r', linestyle='-')\n",
        "    plt.axvline(x=left, color='r', linestyle='-')\n",
        "    plt.axvline(x=right, color='r', linestyle='-')\n",
        "    plt.savefig('./output/ver.png')\n",
        "\n",
        "    # FILTER WITH HORIZONTAL PROJECTION PROFILE ###########################################\n",
        "    # Compute projection profile\n",
        "    hor = np.sum(SMOOTH_IMAGE, axis=1)\n",
        "    # Get all values above average\n",
        "    hor_indexes = np.where(hor > np.mean(hor))\n",
        "    # Find the first and last of them\n",
        "    top = np.min(hor_indexes)\n",
        "    bottom = np.max(hor_indexes)\n",
        "\n",
        "    # Wipe the image on top/bottom sides\n",
        "    image[0:top, :] = 0\n",
        "    image[bottom:, :] = 0\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(hor)\n",
        "    plt.axhline(y=np.mean(hor), color='r', linestyle='-')\n",
        "    plt.axvline(x=top, color='r', linestyle='-')\n",
        "    plt.axvline(x=bottom, color='r', linestyle='-')\n",
        "    plt.savefig('./output/hor.png')\n",
        "    \"\"\"\n",
        "    return image\n",
        "\n",
        "\n",
        "def remove_small_components(image, small_component_ratio):\n",
        "    # Find CC\n",
        "    cc_properties = measure.regionprops(measure.label(image[:, :, 1], background=0), cache=True)\n",
        "\n",
        "    # Compute average metrics\n",
        "    avg_area = np.mean([item.area for item in cc_properties])\n",
        "\n",
        "    # Remove all small components\n",
        "    for cc in cc_properties:\n",
        "        if cc.area < small_component_ratio * avg_area:\n",
        "            # Wipe the cc\n",
        "            image[(cc.coords[:, 0], cc.coords[:, 1])] = 0\n",
        "    return image\n",
        "\n",
        "\n",
        "def remove_big_components(image):\n",
        "    # Find CC\n",
        "    cc_properties = measure.regionprops(measure.label(image[:, :, 1], background=0), cache=True)\n",
        "\n",
        "    # Compute average metrics\n",
        "    avg_area = np.mean([item.area for item in cc_properties])\n",
        "\n",
        "    # Remove all small components\n",
        "    for cc in cc_properties:\n",
        "        if cc.area > 10 * avg_area:\n",
        "            # Wipe the cc\n",
        "            image[(cc.coords[:, 0], cc.coords[:, 1])] = 0\n",
        "    return image\n",
        "\n",
        "\n",
        "\n",
        "######################              BLUR IMAGE\n",
        "import logging\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def blow_up_image(image, seams):\n",
        "    # -------------------------------\n",
        "    start = time.time()\n",
        "    # -------------------------------\n",
        "\n",
        "    # new image\n",
        "    new_image = []\n",
        "\n",
        "    # get the new height of the image and the original one\n",
        "    ori_height, _, _ = image.shape\n",
        "    height = ori_height + len(seams)\n",
        "\n",
        "    seams = np.array(seams)\n",
        "\n",
        "    for i in range(0, image.shape[1]):\n",
        "        col = np.copy(image[:, i])\n",
        "        y_cords_seams = seams[:, i, 1]\n",
        "\n",
        "        seam_nb = 0\n",
        "        for y_seam in y_cords_seams:\n",
        "            col = np.insert(col, y_seam + seam_nb, [0, 0, 0], axis=0)\n",
        "            seam_nb += 1\n",
        "\n",
        "        new_image.append(col)\n",
        "\n",
        "    # -------------------------------\n",
        "    stop = time.time()\n",
        "    logging.info(\"finished after: {diff} s\".format(diff=stop - start))\n",
        "    # -------------------------------\n",
        "\n",
        "    return np.swapaxes(np.asarray(new_image), 0, 1), (((100 / ori_height) * height) - 100) / 100\n",
        "\n",
        "\n",
        "def blur_image(img, save_name=\"blur_image.png\", save=False, show=False, filter_size=1000, horizontal=True):\n",
        "    # motion blur the image\n",
        "    # generating the kernel\n",
        "    kernel_motion_blur = np.zeros((filter_size, filter_size))\n",
        "    if horizontal:\n",
        "        kernel_motion_blur[int((filter_size - 1) / 2), :] = np.ones(filter_size)\n",
        "    else:\n",
        "        kernel_motion_blur[:, int((filter_size - 1) / 2)] = np.ones(filter_size)\n",
        "    kernel_motion_blur = kernel_motion_blur / filter_size\n",
        "\n",
        "    # applying the kernel to the input image\n",
        "    output = cv2.filter2D(img, -1, kernel_motion_blur)\n",
        "\n",
        "    if save:\n",
        "        cv2.imwrite(save_name, output)\n",
        "\n",
        "    if show:\n",
        "        cv2.imshow('image', output)\n",
        "        cv2.waitKey(0)\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "    return output  # , np.sum(output, axis=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xgOoiHG7GJ1C"
      },
      "outputs": [],
      "source": [
        "#@title Energy Map\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "from skimage import measure\n",
        "\n",
        "#import src\n",
        "#from src.line_segmentation.utils.unused_but_keep_them import blur_image\n",
        "#from src.line_segmentation.utils.util import calculate_asymmetric_distance, save_img\n",
        "\n",
        "\n",
        "def create_heat_map_visualization(ori_energy_map):\n",
        "    # -------------------------------\n",
        "    start = time.time()\n",
        "    # -------------------------------\n",
        "\n",
        "    heatmap = ((np.copy(ori_energy_map) / np.max(ori_energy_map)))\n",
        "    heatmap = (np.stack((heatmap,) * 3, axis=-1)) * 255\n",
        "    heatmap = np.array(heatmap, dtype=np.uint8)\n",
        "    # show_img(cv2.applyColorMap(heatmap, cv2.COLORMAP_JET))\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    # result = cv2.add(cv2.applyColorMap(heatmap, cv2.COLORMAP_JET), img)\n",
        "\n",
        "    # -------------------------------\n",
        "    stop = time.time()\n",
        "    logging.info(\"finished after: {diff} s\".format(diff=stop - start))\n",
        "    # -------------------------------\n",
        "\n",
        "    return heatmap\n",
        "\n",
        "\n",
        "def prepare_energy(ori_map, left_column, right_column, y):\n",
        "    \"\"\"\n",
        "    Sets the left and right border of the matrix to int.MAX except at y.\n",
        "\n",
        "    :param ori_map:\n",
        "    :param y:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    y_value_left, y_value_right = left_column[y], right_column[y]\n",
        "    ori_map[:, 0] = sys.maxsize / 2\n",
        "    ori_map[:, -1] = sys.maxsize / 2\n",
        "\n",
        "    ori_map[y][0], ori_map[y][-1] = y_value_left, y_value_right\n",
        "\n",
        "    return ori_map\n",
        "\n",
        "def create_distance_matrix(img_shape, centroids, asymmetric=False, side_length=1000):\n",
        "    # -------------------------------\n",
        "    start = time.time()\n",
        "    # -------------------------------\n",
        "\n",
        "    template = np.zeros((side_length, side_length))\n",
        "    center_template = np.array([[np.ceil(side_length / 2), np.ceil(side_length / 2)]])\n",
        "    pixel_coordinates = np.asarray([[x, y] for x in range(template.shape[0]) for y in range(template.shape[1])])\n",
        "\n",
        "    # calculate distance template\n",
        "    # TODO save template for speed up\n",
        "    if asymmetric:\n",
        "        template = np.array([calculate_asymmetric_distance(center_template, pxl, 1, 10) for pxl in pixel_coordinates]) \\\n",
        "            .flatten().reshape((side_length, side_length))\n",
        "    else:\n",
        "        template = distance.cdist(center_template, pixel_coordinates).flatten().reshape((side_length, side_length))\n",
        "\n",
        "    # show_img(create_heat_map_visualization(template))\n",
        "\n",
        "    distance_matrix = np.ones(img_shape) * np.max(template)\n",
        "    # show_img(create_heat_map_visualization(template))\n",
        "    # template[template > np.ceil(side_length / 2)] = np.max(template) * 2\n",
        "    # round the centroid coordinates to ints to use them as array index\n",
        "    centroids = np.rint(centroids).astype(int)\n",
        "\n",
        "    # for each centroid\n",
        "    for centroid in centroids:\n",
        "        pos_v, pos_h = (centroid - np.ceil(side_length / 2)).astype(int)  # offset\n",
        "        v_range1 = slice(max(0, pos_v), max(min(pos_v + template.shape[0], distance_matrix.shape[0]), 0))\n",
        "        h_range1 = slice(max(0, pos_h), max(min(pos_h + template.shape[1], distance_matrix.shape[1]), 0))\n",
        "\n",
        "        v_range2 = slice(max(0, -pos_v), min(-pos_v + distance_matrix.shape[0], template.shape[0]))\n",
        "        h_range2 = slice(max(0, -pos_h), min(-pos_h + distance_matrix.shape[1], template.shape[1]))\n",
        "\n",
        "        # need max\n",
        "        distance_matrix[v_range1, h_range1] = np.minimum(template[v_range2, h_range2],\n",
        "                                                         distance_matrix[v_range1, h_range1])\n",
        "\n",
        "    # -------------------------------\n",
        "    stop = time.time()\n",
        "    logging.info(\"finished after: {diff} s\".format(diff=stop - start))\n",
        "    # -------------------------------\n",
        "\n",
        "    # distance_matrix = distance_matrix.reshape((centroids.shape[0], img_shape[0] * img_shape[1]))\n",
        "    return distance_matrix.flatten()\n",
        "\n",
        "\n",
        "def create_energy_map(img, blurring=True, projection=True, asymmetric=False):\n",
        "    # -------------------------------\n",
        "    start = time.time()\n",
        "    # -------------------------------\n",
        "    # get the cc, all the centroids and the areas of the cc\n",
        "    cc, centroids, areas = find_cc_centroids_areas(img)\n",
        "\n",
        "    # a list of all the pixels in the image as tuple\n",
        "    centroids = np.asarray([[point[0], point[1]] for point in centroids])\n",
        "\n",
        "    # normalise between 0 nd 1\n",
        "    areas = (areas - np.min(areas)) / (np.max(areas) - np.min(areas))\n",
        "\n",
        "    # bring it between -1 and 1\n",
        "    areas = areas - np.mean(areas)\n",
        "\n",
        "    # make it all negative\n",
        "    areas = - np.abs(areas)\n",
        "\n",
        "    # scale it with punishment\n",
        "    areas *= 500\n",
        "\n",
        "    # creating distance matrix\n",
        "    # pixel_coordinates = np.asarray([[x, y] for x in range(img.shape[0]) for y in range(img.shape[1])])\n",
        "    # distance_matrix = distance.cdist(pixel_coordinates, centroids[0:10])\n",
        "    distance_matrix = create_distance_matrix(img.shape[0:2], centroids, asymmetric=asymmetric)\n",
        "\n",
        "    # scale down the distance\n",
        "    distance_matrix /= 30\n",
        "\n",
        "    # make sure the distance is > 1\n",
        "    distance_matrix += 1\n",
        "\n",
        "    # We give all centroids the same energy (100)\n",
        "    energy_background = ((np.ones(img.shape[0] * img.shape[1]) * 100) / distance_matrix).transpose()\n",
        "\n",
        "    # Get the text location and assign it half the energy\n",
        "    locs = np.array(np.where(img[:, :, 0].reshape(-1) == 0))[0:2, :]\n",
        "    energy_text = energy_background\n",
        "    energy_text[locs] = 0\n",
        "\n",
        "    # sum up the received energy for each pixel\n",
        "    energy_map = energy_background + energy_text\n",
        "    #energy_map = energy_background / 100000\n",
        "    energy_map = energy_map.reshape(img.shape[0:2])\n",
        "\n",
        "    if blurring:\n",
        "        # blur the map\n",
        "        blurred_energy_map = blur_image(img=energy_map, filter_size=300)\n",
        "        energy_map = blurred_energy_map\n",
        "\n",
        "    if projection:\n",
        "        projection_profile = create_projection_profile(energy_map)\n",
        "        # scale it between 0 and max(energy_map) / 2\n",
        "        projection_profile *= np.max(energy_map) / 2\n",
        "\n",
        "        # blur projection profile\n",
        "        projection_matrix = np.zeros(img.shape[0:2])\n",
        "        projection_matrix = (projection_matrix.transpose() + projection_profile).transpose()\n",
        "        projection_matrix = blur_image(projection_matrix, filter_size=1000)\n",
        "\n",
        "        # overlap it with the normal energy map and add the text energy\n",
        "        energy_map = energy_map + projection_matrix\n",
        "\n",
        "    if True:\n",
        "        # Cross-shaped kernel\n",
        "        filter_size_H = img.shape[0]\n",
        "        filter_size_V = img.shape[1]\n",
        "        kernel = np.zeros((filter_size_V, filter_size_H))\n",
        "        kernel[int(filter_size_V/2), :] = 1\n",
        "        kernel[:, int(filter_size_H/2)] = 1\n",
        "\n",
        "        # Apply cross filter\n",
        "        smoothed = cv2.filter2D(energy_map, -1, kernel)\n",
        "\n",
        "        # Smoothing kernel\n",
        "        filter_size_H = 32\n",
        "        filter_size_V = 32\n",
        "        kernel = np.ones((filter_size_V, filter_size_H)) / (filter_size_V*filter_size_H)\n",
        "\n",
        "        # Apply smoothing filter\n",
        "        smoothed = cv2.filter2D(smoothed, -1, kernel)\n",
        "\n",
        "        # Remove the mean and clip at 0\n",
        "        smoothed -= np.mean(smoothed)\n",
        "        smoothed[smoothed < 0] = 0\n",
        "\n",
        "        # Normalize it between 0 and max(energy_map)\n",
        "        smoothed = ((smoothed - np.min(smoothed)) * np.max(energy_map)) / (np.max(smoothed) - np.min(smoothed))\n",
        "\n",
        "        # DEBUG\n",
        "        #heatmap = src.line_segmentation.preprocessing.energy_map.create_heat_map_visualization(smoothed)\n",
        "        #save_img(heatmap, path=os.path.join('./output/energy_map.png'))\n",
        "\n",
        "        # Add it to the energy map\n",
        "        energy_map = energy_map + smoothed\n",
        "\n",
        "    # -------------------------------\n",
        "    stop = time.time()\n",
        "    logging.info(\"finished after: {diff} s\".format(diff=stop - start))\n",
        "    # -------------------------------\n",
        "\n",
        "    return energy_map, cc\n",
        "\n",
        "\n",
        "def create_projection_profile(energy_map):\n",
        "    # creating the horizontal projection profile\n",
        "    pp = np.sum(energy_map, axis=1)\n",
        "    # smoothing it\n",
        "    WINDOW_SIZE = 100\n",
        "    pp = smooth(pp, WINDOW_SIZE)[int(WINDOW_SIZE/2):-int(WINDOW_SIZE/2-1)]\n",
        "    # wipe everything below average\n",
        "    pp -= np.mean(pp)\n",
        "    pp[pp < 0] = 0\n",
        "    # normalize it between 0-1\n",
        "    pp = (pp - np.min(pp)) / (np.max(pp) - np.min(pp))\n",
        "    return pp\n",
        "\n",
        "\n",
        "def smooth(x, window_len=11, window='hanning'):\n",
        "    \"\"\"smooth the data using a window with requested size.\n",
        "\n",
        "    This method is based on the convolution of a scaled window with the signal.\n",
        "    The signal is prepared by introducing reflected copies of the signal\n",
        "    (with the window size) in both ends so that transient parts are minimized\n",
        "    in the begining and end part of the output signal.\n",
        "\n",
        "    input:\n",
        "        x: the input signal\n",
        "        window_len: the dimension of the smoothing window; should be an odd integer\n",
        "        window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\n",
        "            flat window will produce a moving average smoothing.\n",
        "\n",
        "    output:\n",
        "        the smoothed signal\n",
        "\n",
        "    example:\n",
        "\n",
        "    t=linspace(-2,2,0.1)\n",
        "    x=sin(t)+randn(len(t))*0.1\n",
        "    y=smooth(x)\n",
        "\n",
        "    see also:\n",
        "\n",
        "    numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve\n",
        "    scipy.signal.lfilter\n",
        "\n",
        "    TODO: the window parameter could be the window itself if an array instead of a string\n",
        "    NOTE: length(output) != length(input), to correct this: return y[(window_len/2-1):-(window_len/2)] instead of just y.\n",
        "    \"\"\"\n",
        "\n",
        "    if x.ndim != 1:\n",
        "        raise ValueError(\"smooth only accepts 1 dimension arrays.\")\n",
        "\n",
        "    if x.size < window_len:\n",
        "        raise ValueError(\"Input vector needs to be bigger than window size.\")\n",
        "\n",
        "    if window_len < 3:\n",
        "        return x\n",
        "\n",
        "    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
        "        raise ValueError(\"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\")\n",
        "\n",
        "    s = np.r_[x[window_len - 1:0:-1], x, x[-2:-window_len - 1:-1]]\n",
        "    # print(len(s))\n",
        "    if window == 'flat':  # moving average\n",
        "        w = np.ones(window_len, 'd')\n",
        "    else:\n",
        "        w = eval('np.' + window + '(window_len)')\n",
        "\n",
        "    y = np.convolve(w / w.sum(), s, mode='valid')\n",
        "    return y\n",
        "\n",
        "\n",
        "\n",
        "def find_cc_centroids_areas(img):\n",
        "    # -------------------------------\n",
        "    start = time.time()\n",
        "    # -------------------------------\n",
        "    #############################################\n",
        "    # Find CC\n",
        "    cc_labels, cc_properties = get_connected_components(img)\n",
        "\n",
        "    amount_of_properties = 0\n",
        "\n",
        "    # Compute average metrics\n",
        "    avg_area = np.mean([item.area for item in cc_properties])\n",
        "    std_area = np.std([item.area for item in cc_properties])\n",
        "    avg_height = np.mean([item.bbox[2] - item.bbox[0] for item in cc_properties])\n",
        "    avg_width = np.mean([item.bbox[3] - item.bbox[1] for item in cc_properties])\n",
        "\n",
        "    while amount_of_properties != len(cc_properties):\n",
        "        # for _ in range(2):\n",
        "        amount_of_properties = len(cc_properties)\n",
        "        image = img[:, :, 1]\n",
        "        #############################################\n",
        "        # Cut all large components into smaller components\n",
        "        coef = 1.5\n",
        "        for item in cc_properties:\n",
        "            if item.area > coef * avg_area \\\n",
        "                    or item.bbox[2] - item.bbox[0] > coef * avg_height \\\n",
        "                    or item.bbox[3] - item.bbox[1] > coef * avg_width:\n",
        "                v_size = abs(item.bbox[0] - item.bbox[2])\n",
        "                h_size = abs(item.bbox[1] - item.bbox[3])\n",
        "                y1, x1, y2, x2 = item.bbox\n",
        "\n",
        "                if float(h_size) / v_size > 1.5:\n",
        "                    image[y1:y2, np.round((x1 + x2) / 2).astype(int)] = 0\n",
        "                elif float(v_size) / h_size > 1.5:\n",
        "                    image[np.round((y1 + y2) / 2).astype(int), x1:x2] = 0\n",
        "                else:\n",
        "                    # img[np.round((y1 + y2) / 2).astype(int), np.round((x1 + x2) / 2).astype(int)] = 0\n",
        "                    image[y1:y2, np.round((x1 + x2) / 2).astype(int)] = 0\n",
        "                    image[np.round((y1 + y2) / 2).astype(int), x1:x2] = 0\n",
        "\n",
        "        img[:, :, 1] = image\n",
        "\n",
        "        # Re-find CC\n",
        "        cc_labels, cc_properties = get_connected_components(img)\n",
        "        #############################################\n",
        "\n",
        "    # Collect CC centroids\n",
        "    all_centroids = np.asarray([cc.centroid[0:2] for cc in cc_properties])\n",
        "\n",
        "    # Collect CC sizes\n",
        "    all_areas = np.asarray([cc.area for cc in cc_properties])\n",
        "\n",
        "    # Discard outliers & sort\n",
        "    no_outliers = detect_outliers(all_areas, avg_area, std_area)\n",
        "    centroids = all_centroids[no_outliers, :]\n",
        "    filtered_area = all_areas[no_outliers]\n",
        "    all_areas = filtered_area[np.argsort(centroids[:, 0])]\n",
        "    all_centroids = centroids[np.argsort(centroids[:, 0]), :]\n",
        "\n",
        "    # -------------------------------\n",
        "    stop = time.time()\n",
        "    logging.info(\"finished after: {diff} s\".format(diff=stop - start))\n",
        "    # -------------------------------\n",
        "\n",
        "    return (cc_labels, cc_properties), all_centroids, all_areas\n",
        "\n",
        "\n",
        "def get_connected_components(img):\n",
        "    cc_labels = measure.label(img[:, :, 1], background=0)\n",
        "    cc_properties = measure.regionprops(cc_labels, cache=True)\n",
        "    return cc_labels, cc_properties\n",
        "\n",
        "\n",
        "def detect_outliers(area, mean, std):\n",
        "    # -------------------------------\n",
        "    start = time.time()\n",
        "    # -------------------------------\n",
        "    if mean is not None:\n",
        "        mean = np.mean(area)\n",
        "    if std is not None:\n",
        "        std = np.std(area)\n",
        "\n",
        "    #no_outliers = abs(area - mean) < 3 * std\n",
        "    no_outliers = area - 0.25*mean > 0\n",
        "\n",
        "    # -------------------------------\n",
        "    stop = time.time()\n",
        "    logging.info(\"finished after: {diff} s\".format(diff=stop - start))\n",
        "    # -------------------------------\n",
        "\n",
        "    return no_outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HYajkH4PIZgb"
      },
      "outputs": [],
      "source": [
        "#@title Seam Carving\n",
        "import itertools\n",
        "import sys\n",
        "\n",
        "import cv2\n",
        "import numba\n",
        "import numpy as np\n",
        "\n",
        "#import src.line_segmentation\n",
        "\n",
        "\"\"\"\n",
        "    Code from: https://github.com/danasilver/seam-carving/blob/master/seamcarve.py\n",
        "    homepage: http://www.faculty.idc.ac.il/arik/SCWeb/imret/\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@numba.jit()\n",
        "def horizontal_seam(energies, penalty_reduction, bidirectional=False):\n",
        "    \"\"\"\n",
        "    Spawns seams from the left to the right or from both directions. It returns the list of seams as point list.\n",
        "\n",
        "    :param energies: the energy map\n",
        "    :param penalty_reduction: if the penalty_reduction is smaller or equal to 0 we wont apply a penalty reduction\n",
        "    :param bidirectional: if True there will be seams from left to right and right to left, else just from left to right\n",
        "    :return: seams as point list\n",
        "    \"\"\"\n",
        "    height, width = energies.shape[:2]\n",
        "    # the y position we started (needed for the penalty)\n",
        "    ori_y = 0\n",
        "    # the last point we visit\n",
        "    previous = 0\n",
        "    # the points of the seam\n",
        "    # from left to right\n",
        "    seam_forward = []\n",
        "    # from right to left\n",
        "    seam_backward = []\n",
        "\n",
        "    # spawns seams from left to right\n",
        "    for i in range(0, width, 1):\n",
        "        col = energies[:, i]\n",
        "        if i == 0:\n",
        "            ori_y = previous = np.argmin(col)\n",
        "        else:\n",
        "            top = col[previous - 1] if previous - 1 >= 0 else sys.maxsize\n",
        "            middle = col[previous]\n",
        "            bottom = col[previous + 1] if previous + 1 < height else sys.maxsize\n",
        "\n",
        "            if penalty_reduction > 0:\n",
        "                top += ((ori_y - (previous - 1)) ** 2) / penalty_reduction\n",
        "                middle += ((ori_y - previous) ** 2) / penalty_reduction\n",
        "                bottom += + ((ori_y - (previous + 1)) ** 2) / penalty_reduction\n",
        "\n",
        "            previous = previous + np.argmin([top, middle, bottom]) - 1\n",
        "\n",
        "        seam_forward.append([i, previous])\n",
        "\n",
        "    # spawns seams from right to left\n",
        "    if bidirectional:\n",
        "        for i in range(width-1, -1, -1):\n",
        "            col = energies[:, i]\n",
        "            if i == width-1:\n",
        "                ori_y = previous = np.argmin(col)\n",
        "            else:\n",
        "                top = col[previous - 1] if previous - 1 >= 0 else sys.maxsize\n",
        "                middle = col[previous]\n",
        "                bottom = col[previous + 1] if previous + 1 < height else sys.maxsize\n",
        "\n",
        "                if penalty_reduction > 0:\n",
        "                    top += ((ori_y - (previous - 1)) ** 2) / penalty_reduction\n",
        "                    middle += ((ori_y - previous) ** 2) / penalty_reduction\n",
        "                    bottom += + ((ori_y - (previous + 1)) ** 2) / penalty_reduction\n",
        "\n",
        "                previous = previous + np.argmin([top, middle, bottom]) - 1\n",
        "\n",
        "            seam_backward.append([i, previous])\n",
        "\n",
        "    return [seam_forward, seam_backward[::-1]]\n",
        "\n",
        "\n",
        "def draw_seams(img, seams, bidirectional=True):\n",
        "\n",
        "    x_axis = np.expand_dims(np.array(range(0, len(seams[0]))), -1)\n",
        "    seams = [np.concatenate((x, np.expand_dims(seam, -1)), axis=1) for seam, x in zip(seams, itertools.repeat(x_axis))]\n",
        "\n",
        "    for i, seam in enumerate(seams):\n",
        "        # Get the seam from the left [0] and the seam from the right[1]\n",
        "        if bidirectional and i % 2 == 0:\n",
        "            cv2.polylines(img, np.int32([seam]), False, (0, 0, 0), 3)  # Black\n",
        "        else:\n",
        "            cv2.polylines(img, np.int32([seam]), False, (255, 255, 255), 3)  # White\n",
        "\n",
        "\n",
        "def draw_seams_red(img, seams, bidirectional=True):\n",
        "\n",
        "    x_axis = np.expand_dims(np.array(range(0, len(seams[0]))), -1)\n",
        "    seams = [np.concatenate((x, np.expand_dims(seam, -1)), axis=1) for seam, x in zip(seams, itertools.repeat(x_axis))]\n",
        "\n",
        "    for i, seam in enumerate(seams):\n",
        "        # Get the seam from the left [0] and the seam from the right[1]\n",
        "            cv2.polylines(img, np.int32([seam]), False, (0, 0, 255), 3)  # Red\n",
        "\n",
        "\n",
        "def get_seams(ori_energy_map, penalty_reduction, seam_every_x_pxl):\n",
        "    # list with all seams\n",
        "    seams = []\n",
        "    # left most column of the energy map\n",
        "    left_column_energy_map = np.copy(ori_energy_map[:, 0])\n",
        "    # right most column of the energy map\n",
        "    right_column_energy_map = np.copy(ori_energy_map[:, -1])\n",
        "    # show_img(ori_enegery_map)\n",
        "    for seam_at in range(0, ori_energy_map.shape[0], seam_every_x_pxl):\n",
        "        energy_map = prepare_energy(ori_energy_map,left_column_energy_map, right_column_energy_map, seam_at)\n",
        "\n",
        "        seams.extend(horizontal_seam(energy_map, penalty_reduction=penalty_reduction, bidirectional=True))\n",
        "\n",
        "    # strip seams of x coordinate, which is totally useless as the x coordinate is basically the index in the array\n",
        "    seams = np.array([np.array(s)[:, 1] for s in seams])\n",
        "\n",
        "    return seams\n",
        "\n",
        "\n",
        "def post_process_seams(energy_map, seams):\n",
        "    # Check that the seams are as wide as the image\n",
        "    assert energy_map.shape[1] == len(seams[0])\n",
        "\n",
        "    # TODO implement a tabu-list to prevent two seams to repeatedly swap a third seam between them\n",
        "    SAFETY_STOP = 100\n",
        "    iteration = 0\n",
        "    repeat = True\n",
        "    while repeat:\n",
        "\n",
        "        # Safety exit in case of endless loop meeting condition. See above.\n",
        "        iteration += 1\n",
        "        if iteration >= SAFETY_STOP:\n",
        "            break\n",
        "\n",
        "        repeat = False\n",
        "        for index, seam_A in enumerate(seams):\n",
        "            for seam_B in seams[index:]:\n",
        "                # Compute seams overlap\n",
        "                overlap = seam_A - seam_B\n",
        "\n",
        "                # Smooth the overlap\n",
        "                overlap[abs(overlap) < 10] = 0\n",
        "\n",
        "                # Make the two seams really overlap\n",
        "                seam_A[overlap == 0] = seam_B[overlap == 0]\n",
        "\n",
        "                # Find non-zero sequences\n",
        "                sequences = non_zero_runs(overlap)\n",
        "\n",
        "                if len(sequences) > 0:\n",
        "                    for i, sequence in enumerate(sequences):\n",
        "\n",
        "                        target = sequence[1] - sequence[0]\n",
        "\n",
        "                        left = sequence[0] - sequences[i - 1, 1] if i > 0 else sequence[0]\n",
        "                        right = sequences[i + 1, 0] - sequence[1] if i < len(sequences)-1 else energy_map.shape[1] - sequence[1]\n",
        "\n",
        "                        if target > left and target > right:\n",
        "                            continue\n",
        "\n",
        "                        repeat = True\n",
        "\n",
        "                        # Expand the sequence into a range\n",
        "                        sequence = range(*sequence)\n",
        "                        # Compute the seam\n",
        "                        energy_A = measure_energy(energy_map, seam_A, sequence)\n",
        "                        energy_B = measure_energy(energy_map, seam_B, sequence)\n",
        "\n",
        "                        # Remove the weaker seam sequence\n",
        "                        if energy_A > energy_B:\n",
        "                            seam_A[sequence] = seam_B[sequence]\n",
        "                        else:\n",
        "                            seam_B[sequence] = seam_A[sequence]\n",
        "\n",
        "    return seams\n",
        "\n",
        "\n",
        "def non_zero_runs(a):\n",
        "    \"\"\"\n",
        "    Finding the consecutive non-zeros in a numpy array. Modified from:\n",
        "    https://stackoverflow.com/questions/24885092/finding-the-consecutive-zeros-in-a-numpy-array\n",
        "    \"\"\"\n",
        "    # Create an array that is 1 where a is 0, and pad each end with an extra 0.\n",
        "    iszero = np.concatenate(([1], np.equal(a, 0).view(np.int8), [1]))\n",
        "    absdiff = np.abs(np.diff(iszero))\n",
        "    # Runs start and end where absdiff is 1.\n",
        "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
        "    return ranges\n",
        "\n",
        "\n",
        "def measure_energy(energy_map, seam, sequence):\n",
        "    \"\"\"\n",
        "    Compute the energy of that seams for the specified range\n",
        "    \"\"\"\n",
        "    return energy_map[seam[sequence], sequence].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FARgyhBnH7vs"
      },
      "outputs": [],
      "source": [
        "#@title Binning\n",
        "import itertools\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#from src.line_segmentation.seamcarving_algorithm import draw_seams_red\n",
        "#from src.line_segmentation.utils.util import calculate_asymmetric_distance, save_img\n",
        "\n",
        "\n",
        "def majority_voting(connected_components, seams):\n",
        "    \"\"\"\n",
        "    Splits the centroids into bins according to how many seams cross them\n",
        "    \"\"\"\n",
        "    # -------------------------------\n",
        "    start = time.time()\n",
        "    # -------------------------------\n",
        "\n",
        "    # Get the centroids and sort them\n",
        "    centroids = np.asarray([cc.centroid[0:2] for cc in connected_components[1]])\n",
        "    centroids = centroids[np.argsort(centroids[:, 0]), :]\n",
        "\n",
        "    # for each centroid, compute how many seams are above it\n",
        "    values = count_seams_below(centroids, seams)\n",
        "\n",
        "    small_bins = [42]  # Just to enter the while loop once\n",
        "    while len(small_bins) > 0:\n",
        "        # split values into bins index\n",
        "        bin_index, bin_size, unique_bins = split_into_bins_and_index(values)\n",
        "\n",
        "        # look for outliers and merge them into bigger clusters\n",
        "        if small_bins[0] == 42:\n",
        "            avg = np.mean(bin_size[bin_size>1])*0.25\n",
        "\n",
        "            # Compute average centroid horizontal distance:\n",
        "            # distances = []\n",
        "            # for bin in unique_bins:\n",
        "            #     distances.extend(compute_avg_pairwise_distance(centroids[np.where(bin_index == bin)]))\n",
        "            # threshold = 5 * np.mean(distances)\n",
        "            #\n",
        "            # # Scatter bins which have an anomaly in the avg distance\n",
        "            # for bin in unique_bins:\n",
        "            #     locs = np.where(bin_index == bin)\n",
        "            #     if check_for_anomaly(centroids[locs], threshold):\n",
        "            #         # Compute the offset for the scattered bin\n",
        "            #         offset = np.array(range(0, len(locs[0])))\n",
        "            #         # Assign them to the bin, thus scattering it into many single-centroid bins\n",
        "            #         values[locs] -= offset\n",
        "            #         # Get index of next bin\n",
        "            #         nb = int(np.max(locs)) + 1\n",
        "            #         # Adjust following bins accordingly\n",
        "            #         values[nb:] -= np.max(offset)\n",
        "\n",
        "        # Detect clusters which are too small\n",
        "        small_bins = unique_bins[np.where(bin_size < avg)]\n",
        "\n",
        "        # Merge small bins\n",
        "        merge_small_bins(bin_index, centroids, small_bins, values)\n",
        "\n",
        "\n",
        "    # Split the centroids into bins according to the clusters\n",
        "    lines = []\n",
        "    for bin in unique_bins:\n",
        "        lines.append(list(centroids[np.where(bin_index == bin)]))\n",
        "\n",
        "    # -------------------------------\n",
        "    stop = time.time()\n",
        "    logging.info(\"finished after: {diff} s\".format(diff=stop - start))\n",
        "    # -------------------------------\n",
        "\n",
        "    return lines, centroids, values\n",
        "\n",
        "\n",
        "def count_seams_below(centroids, seams):\n",
        "    values = np.zeros([len(centroids)])\n",
        "    for i, centroid in enumerate(centroids):\n",
        "        cx = int(centroid[1])\n",
        "        cy = int(centroid[0])\n",
        "        for seam in seams:\n",
        "            # if the seam is above the centroid at the centroid X position\n",
        "            if seam[cx] > cy:\n",
        "                values[i] = values[i] + 1\n",
        "    return values\n",
        "\n",
        "\n",
        "def merge_small_bins(bin_index, centroids, small_bins, values):\n",
        "    for bin in small_bins:\n",
        "        # find the indexes of the samples which are undernumbered\n",
        "        for loc in np.where(bin_index == bin)[0]:\n",
        "            # look for the next available bin below\n",
        "            loc_p = loc + 1 if loc + 1 < len(values) else loc\n",
        "            while bin_index[loc_p] == bin_index[loc]:\n",
        "                if loc_p + 1 < len(values):\n",
        "                    loc_p += 1\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            # look for the next available bin above\n",
        "            loc_m = loc - 1 if loc > 0 else loc\n",
        "            while bin_index[loc_m] == bin_index[loc]:\n",
        "                if loc_m > 0:\n",
        "                    loc_m -= 1\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            # compute distances to neighbors with the EUC distance\n",
        "            XA = np.expand_dims(centroids[loc], axis=0)\n",
        "\n",
        "            upper = np.array([calculate_asymmetric_distance(XA, c, 1, 5) for c in\n",
        "                              centroids[np.where(bin_index == bin_index[loc_p])]]).min()\n",
        "            lower = np.array([calculate_asymmetric_distance(XA, c, 1, 5) for c in\n",
        "                              centroids[np.where(bin_index == bin_index[loc_m])]]).min()\n",
        "\n",
        "            values[loc] = values[loc_m] if (upper == 0 or upper > lower) and lower != 0 else values[loc_p]\n",
        "\n",
        "\n",
        "def split_into_bins_and_index(values):\n",
        "    # Bin index is a an array with the bin number for each entry in values\n",
        "    bin_index = np.digitize(values, np.unique(values))\n",
        "    # Get the bins values and their size\n",
        "    unique_bins, bin_size = np.unique(bin_index, return_counts=True)\n",
        "    return bin_index, bin_size, unique_bins\n",
        "\n",
        "\n",
        "def compute_avg_pairwise_distance(centroids):\n",
        "    # Sort the centroids based on their x-coordinate\n",
        "    centroids = centroids[np.argsort(centroids[:, 1]), :]\n",
        "    dist = []\n",
        "    for c1, c2 in pairwise(centroids):\n",
        "        # Compute the distance in the horizontal axis\n",
        "        dist.append(c2[1] - c1[1])\n",
        "    return dist\n",
        "\n",
        "\n",
        "def check_for_anomaly(centroids, threshold):\n",
        "    # Sort the centroids based on their x-coordinate\n",
        "    centroids = centroids[np.argsort(centroids[:, 1]), :]\n",
        "    for c1, c2 in pairwise(centroids):\n",
        "        # Compute the distance in the horizontal axis; If it is higher thant a threshold, trigger\n",
        "        if c2[1] - c1[1] > threshold:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def pairwise(iterable):\n",
        "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
        "    a, b = itertools.tee(iterable)\n",
        "    next(b, None)\n",
        "    return zip(a, b)\n",
        "\n",
        "\n",
        "def draw_bins(img, centroids, root_output_path, seams, bins):\n",
        "    # create white image\n",
        "    binning_img = np.zeros(img.shape[0:2], dtype=np.uint8)\n",
        "    binning_img.fill(255)\n",
        "    # get text location\n",
        "    locs = np.array(np.where(img[:, :, 0].reshape(-1) != 0))[0:2, :]\n",
        "    binning_img = binning_img.flatten()\n",
        "    binning_img[locs] = 211\n",
        "    binning_img = binning_img.reshape(img.shape[0:2])\n",
        "    binning_img = np.stack((binning_img,) * 3, axis=-1)\n",
        "    # draw seams\n",
        "    draw_seams_red(binning_img, seams)\n",
        "    overlay_img = binning_img.copy()\n",
        "    # draw the centroids on the seam energy map\n",
        "    for centroid, value in zip(centroids, bins):\n",
        "        cv2.circle(overlay_img, (int(centroid[1]), int(centroid[0])), 25, (0, 255, 0), -1)\n",
        "        cv2.putText(binning_img, str(int(value)), (int(centroid[1]) - 16, int(centroid[0]) + 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2, cv2.LINE_AA)\n",
        "    cv2.addWeighted(overlay_img, 0.4, binning_img, 0.6, 0, binning_img)\n",
        "    save_img(binning_img, path=os.path.join(root_output_path, 'energy_map', 'energy_map_bin_expl.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "w5t_Y4kjKlgD"
      },
      "outputs": [],
      "source": [
        "#@title Polygon Manager\n",
        "import logging\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "import networkx as nx1\n",
        "import numpy as np\n",
        "from skimage import measure\n",
        "\n",
        "#from src.line_segmentation.utils.graph_util import createTINgraph, print_graph_on_img\n",
        "\n",
        "\n",
        "def get_polygons_from_lines(img, lines, connected_components, vertical):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Extract the contour of each CC\n",
        "    polygon_coords = []\n",
        "    for i, line in enumerate(lines):\n",
        "        cc_coords = []\n",
        "        graph_nodes = []\n",
        "        polygon_img = np.zeros(img.shape)\n",
        "        for c in line:\n",
        "            cc = find_cc_from_centroid(c, connected_components[1])\n",
        "            points = cc.coords[::3, 0:2]\n",
        "            points = np.asarray([[point[1], point[0]] for point in points])\n",
        "            cc_coords.append(points)\n",
        "            # add the first contour point to the list s.t. the line will be connected\n",
        "            graph_nodes.append(find_graph_node(cc.coords, cc.centroid))\n",
        "\n",
        "        # create graph\n",
        "        overlay_graph = createTINgraph(np.array(list(set(tuple(p) for p in graph_nodes))))\n",
        "\n",
        "        # create mst\n",
        "        overlay_graph = nx.minimum_spanning_tree(overlay_graph)\n",
        "\n",
        "        # overlay\n",
        "        polygon_img = print_graph_on_img(polygon_img, [overlay_graph], color=(255, 255, 255), thickness=1)\n",
        "        cv2.fillPoly(polygon_img, cc_coords, color=(255, 255, 255))\n",
        "\n",
        "        # for cc_coord in cc_coords:\n",
        "        #     # add cc areas to the image\n",
        "        #     cv2.fillPoly(polygon_img, cc_coord, color=(255, 255, 255))\n",
        "\n",
        "        if vertical:\n",
        "            polygon_img = cv2.rotate(polygon_img, cv2.ROTATE_90_CLOCKWISE)\n",
        "\n",
        "        # blurring the polygon image to close the gaps between the cut CCs\n",
        "        filter_size_H = 5\n",
        "        filter_size_V = 5\n",
        "        kernel = np.ones((filter_size_V, filter_size_H)) / filter_size_H\n",
        "        # Apply averaging filter\n",
        "        polygon_img = cv2.filter2D(polygon_img, -1, kernel)\n",
        "\n",
        "        # get contour points of the binary polygon image\n",
        "        polygon_coords.append(measure.find_contours(polygon_img[:, :, 0], 5, fully_connected='high')[0])\n",
        "\n",
        "    return polygon_coords\n",
        "\n",
        "\n",
        "def find_graph_node(coords, centroid):\n",
        "    # cast centroid coordinates into int\n",
        "    centroid = np.asarray(centroid, dtype=int)\n",
        "\n",
        "    # if centroid is in the coords we return the centroid\n",
        "    if centroid in coords:\n",
        "        return centroid\n",
        "\n",
        "    # get the extreme points in coords on the same y as centroid\n",
        "    # extrem_points = coords[np.where(coords[:, 1] == centroid[1])]\n",
        "\n",
        "    return [coords[0][0], coords[0][1]]\n",
        "\n",
        "\n",
        "def find_cc_from_centroid(c, cc_properties):\n",
        "    #   c[0], c[1] = c[1], c[0]\n",
        "    for cc in cc_properties:\n",
        "        if cc.centroid[0] == c[0] and cc.centroid[1] == c[1]:\n",
        "            return cc\n",
        "    print(\"If this is printed, you might want to uncomment the line swapping the coordinates!\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def draw_polygons(image, polygons, vertical):\n",
        "    if vertical:\n",
        "        image = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
        "\n",
        "    for polygon in polygons:\n",
        "        cv2.polylines(image, np.array([[[np.int(p[1]), np.int(p[0])] for p in polygon]]), 1, color=(248, 24, 148), thickness=3)\n",
        "    return image\n",
        "\n",
        "\n",
        "def polygon_to_string(polygons):\n",
        "    # -------------------------------\n",
        "    start = time.time()\n",
        "    # -------------------------------\n",
        "    strings = []\n",
        "    for polygon in polygons:\n",
        "        line_string = []\n",
        "        for i, point in enumerate(polygon):\n",
        "            if i % 3 != 0:\n",
        "                continue\n",
        "            line_string.append(\"{},{}\".format(int(point[1]), int(point[0])))\n",
        "        strings.append(' '.join(line_string))\n",
        "\n",
        "    # -------------------------------\n",
        "    stop = time.time()\n",
        "    logging.info(\"finished after: {diff} s\".format(diff=stop - start))\n",
        "    # -------------------------------\n",
        "    return strings"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b7r4QVLF7gF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M016Y7nxJMLM"
      },
      "outputs": [],
      "source": [
        "#@title Util - XML\n",
        "import datetime\n",
        "import re\n",
        "import xml.dom.minidom as minidom\n",
        "import xml.etree.cElementTree as ET\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def writePAGEfile(output_path, text_lines=\"\", text_region_coords=\"not provided\", baselines=None):\n",
        "    # Create root element and add the attributes\n",
        "    root = ET.Element(\"PcGts\")\n",
        "    root.set(\"xmls\", \"http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15\")\n",
        "    root.set(\"xmlns:xsi\", \"http://www.w3.org/2001/XMLSchema-instance\")\n",
        "    root.set(\"xsi:schemaLocation\", \"http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15 http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15/pagecontent.xsd\")\n",
        "\n",
        "    # Add metadata\n",
        "    metadata = ET.SubElement(root, \"Metadata\")\n",
        "    ET.SubElement(metadata, \"Creator\").text = \"Michele Alberti, Vinaychandran Pondenkandath\"\n",
        "    ET.SubElement(metadata, \"Created\").text = datetime.datetime.now().strftime('%Y/%m/%d %H:%M:%S')\n",
        "    ET.SubElement(metadata, \"LastChange\").text = datetime.datetime.now().strftime('%Y/%m/%d %H:%M:%S')\n",
        "\n",
        "    # Add page\n",
        "    page = ET.SubElement(root, \"Page\")\n",
        "\n",
        "    # Add TextRegion\n",
        "    textRegion = ET.SubElement(page, \"TextRegion\")\n",
        "    textRegion.set(\"id\", \"region_textline\")\n",
        "    textRegion.set(\"custom\", \"0\")\n",
        "\n",
        "    # Add Coords\n",
        "    ET.SubElement(textRegion, \"Coords\", points=text_region_coords)\n",
        "\n",
        "    # Add TextLine\n",
        "    for i, line in enumerate(text_lines):\n",
        "        textLine = ET.SubElement(textRegion, \"TextLine\", id=\"textline_{}\".format(i), custom=\"0\")\n",
        "        ET.SubElement(textLine, \"Coords\", points=line)\n",
        "        if baselines:\n",
        "            ET.SubElement(textLine, \"Baseline\", points=baselines[i])\n",
        "        else:\n",
        "            ET.SubElement(textLine, \"Baseline\", points=\"not provided\")\n",
        "        textEquiv = ET.SubElement(textLine, \"TextEquiv\")\n",
        "        ET.SubElement(textEquiv, \"Unicode\")\n",
        "\n",
        "    # Add TextEquiv to textRegion\n",
        "    textEquiv = ET.SubElement(textRegion, \"TextEquiv\")\n",
        "    ET.SubElement(textEquiv, \"Unicode\")\n",
        "\n",
        "    #print(prettify(root))\n",
        "\n",
        "    # Save on file\n",
        "    file = open(output_path, \"w\")\n",
        "    file.write(prettify(root))\n",
        "    file.close()\n",
        "\n",
        "\n",
        "def read_max_textline_from_file(pageFile):\n",
        "    tree = ET.parse(pageFile)\n",
        "    root = tree.getroot()\n",
        "    NSMAP = {'pr': 'http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15'}\n",
        "    id = 0\n",
        "    for textregion in root[1].findall('.//pr:TextRegion', namespaces=NSMAP):\n",
        "        if 'textline' in textregion.attrib['id']:\n",
        "            for textline in textregion.findall('.//pr:TextLine', namespaces=NSMAP):\n",
        "                str = textline.attrib['id']\n",
        "                id = np.max([id, int(re.findall(r'\\d+', str)[0])])\n",
        "    return id+1\n",
        "\n",
        "\n",
        "def prettify(elem):\n",
        "    \"\"\"Return a pretty-printed XML string for the Element.\n",
        "    \"\"\"\n",
        "    rough_string = ET.tostring(elem, 'utf-8')\n",
        "    reparsed = minidom.parseString(rough_string)\n",
        "    return reparsed.toprettyxml(indent=\"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jonmgEjM75w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pq4f4cnvFw4b"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def create_folder_structure(input_file, output_path, params):\n",
        "    \"\"\"\n",
        "    Creates the following folder structure:\n",
        "    inputfilename_params\n",
        "        - graph\n",
        "        - histo\n",
        "\n",
        "    :param input_file:\n",
        "    :param output_path:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    fileName = os.path.basename(input_file).split('.')[0]\n",
        "\n",
        "    # If the output_path does not exist (the output folder typically) then create it\n",
        "    if not os.path.exists(output_path):\n",
        "        os.mkdir(output_path)\n",
        "\n",
        "    # basefolder\n",
        "    basefolder_path = os.path.join(output_path, fileName + '_penalty_reduction_{}_seams_{}_component_ratio_{}'.format(*params))\n",
        "    # create basefolder\n",
        "    if not os.path.exists(basefolder_path):\n",
        "        os.mkdir(basefolder_path)\n",
        "        # create energy maps folder\n",
        "        os.mkdir(os.path.join(basefolder_path, 'energy_map'))\n",
        "        # create histo folder\n",
        "        os.mkdir(os.path.join(basefolder_path, 'logs'))\n",
        "        # create preprocess folder\n",
        "        os.mkdir(os.path.join(basefolder_path, 'preprocess'))\n",
        "\n",
        "    return basefolder_path\n",
        "\n",
        "\n",
        "def save_img(img, path='experiment.png', show=False):\n",
        "    if show:\n",
        "        cv2.imshow('img', img)\n",
        "        cv2.waitKey(0)\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "    # Save the image at the given path\n",
        "    cv2.imwrite(path, img)\n",
        "\n",
        "\n",
        "def calculate_asymmetric_distance(x, y, h_weight=1, v_weight=5):\n",
        "    return [np.sqrt((((y[0] - x[0][0]) ** 2) * v_weight + ((y[1] - x[0][1]) ** 2) * h_weight)/ (h_weight+v_weight))]\n",
        "\n",
        "\n",
        "def dict_to_string(dictionay):\n",
        "    string = []\n",
        "    for entry in dictionay.items():\n",
        "        string.append('_'.join(entry))\n",
        "    return '_'.join(string)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f9Bpty2p8Cfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6V1HudXWXR3d"
      },
      "outputs": [],
      "source": [
        "#@title Util - Graph\n",
        "import bisect\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "from scipy.spatial import Delaunay\n",
        "from shapely.geometry import LineString\n",
        "\n",
        "#from src.line_segmentation.utils.util import save_img\n",
        "\n",
        "\n",
        "# Lars\n",
        "# triangulate the CC; transform into a graph\n",
        "# graph = createTINgraph(centroids)\n",
        "#\n",
        "# # use the seams to cut them into graphs\n",
        "# graphs = cut_graph_with_seams(graph, last_seams, too_small_pc)\n",
        "#\n",
        "# GraphLogger.draw_graphs(img, graphs, name='cut_graph.png')\n",
        "#\n",
        "\n",
        "\n",
        "def createTINgraph(points):\n",
        "    \"\"\"\n",
        "    http://ssrebelious.blogspot.com/2014/11/how-to-create-delauney-triangulation.html\n",
        "\n",
        "    Creates a graph based on Delaney triangulation\n",
        "\n",
        "    @param points: list of points\n",
        "    @return - a graph made from a Delauney triangulation\n",
        "\n",
        "    @Copyright notice: this code is an improved (by Yury V. Ryabov, 2014, riabovvv@gmail.com) version of\n",
        "                      Tom's code taken from this discussion\n",
        "                      https://groups.google.com/forum/#!topic/networkx-discuss/D7fMmuzVBAw\n",
        "    \"\"\"\n",
        "    # -------------------------------\n",
        "    start = time.time()\n",
        "    # -------------------------------\n",
        "\n",
        "    TIN = Delaunay(points)\n",
        "    edges = set()\n",
        "    # for each Delaunay triangle\n",
        "    for n in range(TIN.nsimplex):\n",
        "        # for each edge of the triangle\n",
        "        # sort the vertices\n",
        "        # (sorting avoids duplicated edges being added to the set)\n",
        "        # and add to the edges set\n",
        "        edge = sorted([TIN.vertices[n, 0], TIN.vertices[n, 1]])\n",
        "        # TODO weighted eucleadean distance measure\n",
        "        edges.add((edge[0], edge[1], asymetric_distance(edge, points)))\n",
        "        edge = sorted([TIN.vertices[n, 0], TIN.vertices[n, 2]])\n",
        "        edges.add((edge[0], edge[1], asymetric_distance(edge, points)))\n",
        "        edge = sorted([TIN.vertices[n, 1], TIN.vertices[n, 2]])\n",
        "        edges.add((edge[0], edge[1], asymetric_distance(edge, points)))\n",
        "\n",
        "    # make a graph based on the Delaunay triangulation edges\n",
        "    graph = nx.Graph()\n",
        "    graph.add_weighted_edges_from(edges)\n",
        "\n",
        "    original_nodes = points\n",
        "    assert len(original_nodes) == graph.number_of_nodes()\n",
        "\n",
        "    # create attribute dict\n",
        "    attributes = {}\n",
        "    for n in range(len(original_nodes)):\n",
        "        XY = original_nodes[n]  # X and Y tuple - coordinates of the original points\n",
        "        attributes[n] = [XY[1], XY[0]]\n",
        "\n",
        "    # add attributes to graph\n",
        "    nx.set_node_attributes(graph, attributes, 'XY')\n",
        "\n",
        "    # -------------------------------\n",
        "    stop = time.time()\n",
        "    logging.info(\"finished after: {diff} s\".format(diff=stop - start))\n",
        "    # -------------------------------\n",
        "\n",
        "    return graph\n",
        "\n",
        "\n",
        "def asymetric_distance(edge, points):\n",
        "    return np.linalg.norm(\n",
        "        np.asarray(points[edge[0]]) * np.array([1, 3]) - np.asarray(points[edge[1]]) * np.array([1, 3]))\n",
        "\n",
        "\n",
        "def print_graph_on_img(img, graphs, color=(0, 255, 0), thickness=3):\n",
        "    img = img.copy()\n",
        "    for graph in graphs:\n",
        "        for edge in graph.edges:\n",
        "            p1, p2 = get_edge_node_coordinates(edge, graph)\n",
        "            cv2.line(img, p1, p2, color, thickness=thickness)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def get_edge_node_coordinates(edge, graph):\n",
        "    # node attributes (in our case the XY attribute)\n",
        "    node_attributes = nx.get_node_attributes(graph, 'XY')\n",
        "\n",
        "    p1 = np.asarray(node_attributes[edge[0]], dtype=np.uint32)\n",
        "    p1 = (p1[0], p1[1])\n",
        "    p2 = np.asarray(node_attributes[edge[1]], dtype=np.uint32)\n",
        "    p2 = (p2[0], p2[1])\n",
        "    return p1, p2\n",
        "\n",
        "\n",
        "def cut_graph_with_seams(graph, seams, too_small_pc):\n",
        "    # -------------------------------\n",
        "    start = time.time()\n",
        "    # -------------------------------\n",
        "\n",
        "    tic = time.time()\n",
        "    unique_edges, weights, occurrences = find_intersected_edges(graph, seams)\n",
        "    logging.info(\"find_intersected_edges: {}\".format(time.time()-tic))\n",
        "\n",
        "    # create histogram and save it\n",
        "    # plt.hist(occurrences, bins='auto')\n",
        "    # plt.savefig(os.path.join(output_path, 'histo/histogram_of_intersection_occurrences.png'))\n",
        "\n",
        "    # remove the edges from the graph\n",
        "    graph.remove_edges_from(unique_edges)\n",
        "\n",
        "    if nx.is_connected(graph):\n",
        "        return list([graph])\n",
        "\n",
        "    # get the graphs\n",
        "    graphs = np.asarray(list(nx.connected_component_subgraphs(graph)))\n",
        "    # detect the small ones\n",
        "    small_graphs = detect_small_graphs(graphs, too_small_pc).tolist()\n",
        "\n",
        "    # check if there are small graphs\n",
        "    while small_graphs:\n",
        "        # merge the small graphs\n",
        "        graph = merge_small_graphs(graph, list(small_graphs), unique_edges, weights)\n",
        "        # get the graphs\n",
        "        graphs = np.asarray(list(nx.connected_component_subgraphs(graph)))\n",
        "        # detect the small ones\n",
        "        small_graphs = detect_small_graphs(graphs, too_small_pc)\n",
        "\n",
        "    # check again if the graph is still connected\n",
        "    if nx.is_connected(graph):\n",
        "        return list([graph])\n",
        "\n",
        "    # -------------------------------\n",
        "    stop = time.time()\n",
        "    logging.info(\"finished after: {diff} s\".format(diff=stop - start))\n",
        "    # -------------------------------\n",
        "\n",
        "    return graphs\n",
        "\n",
        "\n",
        "def get_neighbouring_seams_index(seams_max_y, seams_min_y, edge_max_y, edge_min_y):\n",
        "    \"\"\"\n",
        "    Get the indexes of the seams which could possibly intersect with the point\n",
        "    \"\"\"\n",
        "    # reads as \"the highest (in terms of value, don't forget top left is 0,0!) value of\n",
        "    # a seam has to be higher than the lowest value of the edge, and, the lowest point\n",
        "    # of the seams has to be lower than the highest of the edge. In this way there CAN\n",
        "    # be an intersection, but outside this range its mathematically impossible that there is.\n",
        "\n",
        "    # IMPORTANT: note that seams_max_y and seams_min_y MUST be sorted. Here, we don't\n",
        "    # sort them because of their sorted nature. As a matter of fact, it is impossible\n",
        "    # for a seam to cross another one; if they meet the merge but one will never go on \"the\n",
        "    # other side\" because of obvious reasons but most importantly the penalty. So the lists\n",
        "    # are for us already sorted.\n",
        "    return [bisect.bisect_left(seams_max_y, edge_min_y), bisect.bisect_left(seams_min_y, edge_max_y)]\n",
        "\n",
        "\n",
        "def chunks(l, n):\n",
        "    \"\"\"Yield successive len(l)/n-sized chunks from l.\"\"\"\n",
        "    # ensure we dont ask for too many chunks\n",
        "    n = np.min([n, len(l)])\n",
        "    # get the size of each chunk\n",
        "    size  = int(np.ceil(len(l)/n))\n",
        "    for i in range(0, len(l), size):\n",
        "        yield l[i:i + size]\n",
        "\n",
        "\n",
        "def find_intersected_edges(graph, seams):\n",
        "    # strip seams of x coordinate, which is totally useless as the x coordinate is basically the index in the array\n",
        "    seams_y = [np.array(s)[:, 1] for s in seams]\n",
        "    seams_max_y = np.max(seams_y, axis=1)\n",
        "    seams_min_y = np.min(seams_y, axis=1)\n",
        "\n",
        "    # node attributes (in our case the XY attribute)\n",
        "    node_attributes = nx.get_node_attributes(graph, 'XY')\n",
        "\n",
        "    # compute the lines out of seams\n",
        "    seams = [LineString(seam) for seam in seams]\n",
        "\n",
        "    # make the data iterable/splittable\n",
        "    edges = [e for e in graph.edges]\n",
        "    edges.sort(key=lambda tup: tup[0])\n",
        "\n",
        "    edges_to_remove = []\n",
        "    # split the data into chunks\n",
        "    for chunk in chunks(edges, 250):\n",
        "        # select the max/min of the edges in this chunk\n",
        "        tmp = np.array(chunk)\n",
        "        p1 = np.array([node_attributes[edge[0]] for edge in tmp])\n",
        "        p2 = np.array([node_attributes[edge[1]] for edge in tmp])\n",
        "        edge_max_y = np.max((p1[:, 1], p2[:, 1]))\n",
        "        edge_min_y = np.min((p1[:, 1], p2[:, 1]))\n",
        "        # get the possible seams to intersect\n",
        "        index = get_neighbouring_seams_index(seams_max_y, seams_min_y, edge_max_y, edge_min_y)\n",
        "        # for each edge in the chunk check if it actually does intersect\n",
        "        for start, end, edge in zip(p1, p2, chunk):\n",
        "            line_edge = LineString([start, end])\n",
        "            for seam in seams[index[0]:index[1]]:\n",
        "                if line_edge.intersects(seam):\n",
        "                    edges_to_remove.append(edge)\n",
        "                    break\n",
        "\n",
        "    # # LEGACY CODE - not optimized but readable\n",
        "    # edges_to_remove = []\n",
        "    # for edge in graph.edges:\n",
        "    #     p1 = np.asarray(node_attributes[edge[0]], dtype=np.uint32)\n",
        "    #     p2 = np.asarray(node_attributes[edge[1]], dtype=np.uint32)\n",
        "    #     line_edge =  LineString([p1, p2])\n",
        "    #     for seam in seams:\n",
        "    #         if line_edge.intersects(seam):\n",
        "    #             edges_to_remove.append(edge)\n",
        "    #             break\n",
        "\n",
        "    # getting unique edges and counting them (how many times they where hit by a seam)\n",
        "    unique_edges, occurrences = np.unique(np.array(edges_to_remove), return_counts=True, axis=0)\n",
        "    weights = [graph.edges[edge]['weight'] for edge in unique_edges]\n",
        "\n",
        "    return unique_edges, weights, occurrences\n",
        "\n",
        "\n",
        "def merge_small_graphs(graph, small_graphs, unique_edges, weights):\n",
        "    # list of edges to restore in the graph\n",
        "    edges_to_add = []\n",
        "    weights = np.asarray(weights)\n",
        "\n",
        "    # TODO based on distance\n",
        "    # merging small graph\n",
        "    while small_graphs:\n",
        "        # the indices of all edges which where cut from the given (small)graph\n",
        "        small_graph = small_graphs.pop()\n",
        "        edge_idxs = np.unique(np.hstack(np.asarray(\n",
        "            [np.where(unique_edges == node)[0] for node in list(small_graph.nodes)])))\n",
        "        # find the index in the in th edge index array\n",
        "        min_edge_idx = edge_idxs[np.argmin(weights[edge_idxs])]\n",
        "        # get edge to restore and add it to the list of edges to add\n",
        "        edge = unique_edges[min_edge_idx]\n",
        "        unique_edges = np.delete(unique_edges, min_edge_idx, axis=0)\n",
        "        weights = np.delete(weights, min_edge_idx, axis=0)\n",
        "        edges_to_add.append((edge[0], edge[1], weights[min_edge_idx]))\n",
        "    # add again the edges\n",
        "    graph.add_weighted_edges_from(edges_to_add)\n",
        "    return graph\n",
        "\n",
        "\n",
        "def graph_to_point_lists(graphs):\n",
        "    return [list(nx.get_node_attributes(graph, 'XY').values()) for graph in graphs]\n",
        "\n",
        "\n",
        "def detect_small_graphs(graphs, too_small_pc):\n",
        "    # -------------------------------\n",
        "    start = time.time()\n",
        "    # -------------------------------\n",
        "\n",
        "    graph_sizes = np.asarray([len(g.nodes) for g in graphs])\n",
        "    # threshold which graphs are considered as small\n",
        "    too_small = graph_sizes < too_small_pc * np.mean(graph_sizes)\n",
        "\n",
        "    # -------------------------------\n",
        "    stop = time.time()\n",
        "    logging.info(\"finished after: {diff} s\".format(diff=stop - start))\n",
        "    # -------------------------------\n",
        "    # return graphs[too_small]\n",
        "    return graphs[too_small]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yZnzJ8bJ8JzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SG3hDXU_JosD"
      },
      "outputs": [],
      "source": [
        "#@title Util - Graphlogger\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#from src.line_segmentation.utils.graph_util import get_edge_node_coordinates\n",
        "#from src.line_segmentation.utils.util import save_img\n",
        "\n",
        "\n",
        "class GraphLogger:\n",
        "    IMG_SHAPE = ()\n",
        "    ROOT_OUTPUT_PATH = ''\n",
        "\n",
        "    @classmethod\n",
        "    def draw_graphs(cls, img, graphs, color=(0, 255, 0), thickness=3, name='graph.png'):\n",
        "        if not list(img):\n",
        "            img = np.zeros(cls.IMG_SHAPE)\n",
        "        else:\n",
        "            img = img.copy()\n",
        "\n",
        "        for graph in graphs:\n",
        "            img = cls.draw_graph(img, graph, color, thickness, False)\n",
        "\n",
        "        save_img(img, path=os.path.join(cls.ROOT_OUTPUT_PATH, 'graph', name), show=False)\n",
        "\n",
        "        return img\n",
        "\n",
        "    @classmethod\n",
        "    def draw_graph(cls, img, graph, color=(0, 255, 0), thickness=3, save=False, name='graph.png'):\n",
        "        if not list(img):\n",
        "            img = np.zeros(cls.IMG_SHAPE)\n",
        "        else:\n",
        "            img = img.copy()\n",
        "\n",
        "        cls.draw_edges(img, graph.edges, graph, color, thickness, save=False)\n",
        "\n",
        "        if save:\n",
        "            save_img(img, path=os.path.join(cls.ROOT_OUTPUT_PATH, 'graph', name), show=False)\n",
        "\n",
        "        return img\n",
        "\n",
        "    @classmethod\n",
        "    def draw_edges(cls, img, edges, graph, color, thickness, save=False, name='graph.png'):\n",
        "        for edge in edges:\n",
        "            p1, p2 = get_edge_node_coordinates(edge, graph)\n",
        "            cv2.line(img, p1, p2, color, thickness=thickness)\n",
        "\n",
        "        if save:\n",
        "            save_img(img, path=os.path.join(cls.ROOT_OUTPUT_PATH, 'graph', name), show=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload Image\n",
        "from google.colab import files\n",
        "img = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "Tf7Qk6UWXHI5",
        "outputId": "ae7057a8-f060-4d38-b907-7d49a74cc5f5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2cf875da-f1ac-4cd3-b36c-63b88d859d90\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2cf875da-f1ac-4cd3-b36c-63b88d859d90\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test1.png to test1 (4).png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "r6vIVoicLASg"
      },
      "outputs": [],
      "source": [
        "#@title Text Line Segmentation\n",
        "# Utils\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "\n",
        "#import src.line_segmentation.preprocessing.energy_map\n",
        "#from src.line_segmentation.bin_algorithm import majority_voting, draw_bins\n",
        "#from src.line_segmentation.polygon_manager import polygon_to_string, get_polygons_from_lines\n",
        "#from src.line_segmentation.preprocessing.load_image import prepare_image, load_image\n",
        "#from src.line_segmentation.preprocessing.preprocess import preprocess\n",
        "#from src.line_segmentation.seamcarving_algorithm import draw_seams, get_seams, post_process_seams, draw_seams_red\n",
        "#from src.line_segmentation.utils.XMLhandler import writePAGEfile\n",
        "#from src.line_segmentation.utils.graph_logger import GraphLogger\n",
        "#from src.line_segmentation.utils.util import create_folder_structure, save_img\n",
        "\n",
        "\n",
        "#######################################################################################################################\n",
        "\n",
        "\n",
        "def extract_textline(input_path: str, output_path: str, penalty_reduction: int, seam_every_x_pxl: int,\n",
        "                     testing: bool, vertical: bool, console_log: bool, small_component_ratio: float):\n",
        "    \"\"\"\n",
        "    Function to compute the text lines from a segmented image. This is the main routine where the magic happens\n",
        "    \"\"\"\n",
        "\n",
        "    # -------------------------------\n",
        "    start_whole = time.time()\n",
        "    # -------------------------------\n",
        "\n",
        "    ###############################################################################################\n",
        "    # Load the image\n",
        "    img = load_image(input_path)\n",
        "\n",
        "    ###############################################################################################\n",
        "    # Creating the folders and getting the new root folder\n",
        "    root_output_path = create_folder_structure(input_path, output_path, (penalty_reduction, seam_every_x_pxl, small_component_ratio))\n",
        "\n",
        "    # Init the logger with the logging path\n",
        "    init_logger(root_output_path, console_log)\n",
        "\n",
        "    # Init the graph logger\n",
        "    GraphLogger.IMG_SHAPE = img.shape\n",
        "    GraphLogger.ROOT_OUTPUT_PATH = root_output_path\n",
        "\n",
        "    ###############################################################################################\n",
        "    # Prepare image (filter only text, ...)\n",
        "    img = prepare_image(img, testing=testing, cropping=False, vertical=vertical)\n",
        "    # Pre-process the image\n",
        "    save_img(img, path=os.path.join(root_output_path, 'preprocess', 'original.png'))\n",
        "    img = preprocess(img, small_component_ratio)\n",
        "    save_img(img, path=os.path.join(root_output_path, 'preprocess', 'after_preprocessing.png'))\n",
        "\n",
        "  ###############################################################################################\n",
        "    # Create the energy map\n",
        "    energy_map, connected_components = create_energy_map(img,blurring=False,projection=False, asymmetric=True)\n",
        "    # Visualize the energy map as heatmap\n",
        "    heatmap =create_heat_map_visualization(energy_map)\n",
        "    save_img(heatmap, path=os.path.join(root_output_path, 'energy_map', 'energy_map_without_seams.png'))\n",
        "\n",
        "    ###############################################################################################\n",
        "    # Get the seams\n",
        "    seams = get_seams(energy_map, penalty_reduction, seam_every_x_pxl)\n",
        "    # Draw the seams on the heatmap\n",
        "    draw_seams(heatmap, seams)\n",
        "    save_img(heatmap, path=os.path.join(root_output_path, 'energy_map', 'energy_map_with_seams.png'))\n",
        "    # Post-process the seams\n",
        "    seams = post_process_seams(energy_map, seams)\n",
        "    # Draw the seams on the heatmap\n",
        "    draw_seams_red(heatmap, seams)\n",
        "    save_img(heatmap, path=os.path.join(root_output_path, 'energy_map', 'energy_map_postprocessed_seams.png'))\n",
        "\n",
        "    ###############################################################################################\n",
        "    # Extract the bins\n",
        "    lines, centroids, values = majority_voting(connected_components, seams)\n",
        "\n",
        "    # Draw the bins on a white gray image of the text with red seams\n",
        "    draw_bins(img, centroids, root_output_path, seams, values)\n",
        "    ###############################################################################################\n",
        "    # Get polygons from lines\n",
        "    polygons = get_polygons_from_lines(img, lines, connected_components, vertical)\n",
        "    # Draw polygons overlay on original image\n",
        "    save_img(draw_polygons(img.copy(), polygons, vertical), path=os.path.join(root_output_path, 'polygons_on_text.png'))\n",
        "\n",
        "    ###############################################################################################\n",
        "    # Write the results on the XML file\n",
        "    writePAGEfile(os.path.join(root_output_path, 'polygons.xml'), polygon_to_string(polygons))\n",
        "\n",
        "    ###############################################################################################\n",
        "    # -------------------------------\n",
        "    stop_whole = time.time()\n",
        "    logging.info(\"finished after: {diff} s\".format(diff=stop_whole - start_whole))\n",
        "    # -------------------------------'''\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "#######################################################################################################################\n",
        "def init_logger(root_output_path, console_log):\n",
        "    # create a logging format\n",
        "    formatter = logging.Formatter(fmt='%(asctime)s %(filename)s:%(funcName)s %(levelname)-8s %(message)s',\n",
        "                                  datefmt='%Y-%m-%d %H:%M:%S')\n",
        "    # get the logger\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(logging.INFO)\n",
        "    # create and add file handler\n",
        "    handler = logging.FileHandler(os.path.join(root_output_path, 'logs', 'extract_textline.log'))\n",
        "    handler.setLevel(logging.INFO)\n",
        "    handler.setFormatter(formatter)\n",
        "    logger.addHandler(handler)\n",
        "    if console_log:\n",
        "        # create and add stderr handler\n",
        "        stderr_handler = logging.StreamHandler()\n",
        "        stderr_handler.formatter = formatter\n",
        "        logger.addHandler(stderr_handler)\n",
        "\n",
        "#######################################################################################################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KATomXah8iEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3ociBZONqWx",
        "outputId": "02b4d8c6-3750-4a96-9cc9-3c88cab218ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:finished after: 0.07800126075744629 s\n",
            "INFO:root:finished after: 1.5886080265045166 s\n",
            "INFO:root:finished after: 0.00021719932556152344 s\n",
            "INFO:root:finished after: 0.21899199485778809 s\n",
            "INFO:root:finished after: 17.893927335739136 s\n",
            "INFO:root:finished after: 18.9797146320343 s\n",
            "INFO:root:finished after: 0.05893683433532715 s\n",
            "<ipython-input-5-d01188e81256>:17: NumbaWarning: \n",
            "Compilation is falling back to object mode WITH looplifting enabled because Function \"horizontal_seam\" failed type inference due to: No implementation of function Function(<function argmin at 0x7dd047953010>) found for signature:\n",
            " \n",
            " >>> argmin(list(float64)<iv=None>)\n",
            " \n",
            "There are 2 candidate implementations:\n",
            "     - Of which 2 did not match due to:\n",
            "     Overload in function 'array_argmin': File: numba/np/arraymath.py: Line 706.\n",
            "       With argument(s): '(list(float64)<iv=None>)':\n",
            "      Rejected as the implementation raised a specific error:\n",
            "        TypingError: Failed in nopython mode pipeline (step: nopython frontend)\n",
            "      No implementation of function Function(<function array_argmin_impl_float at 0x7dcfed618160>) found for signature:\n",
            "       \n",
            "       >>> array_argmin_impl_float(list(float64)<iv=None>)\n",
            "       \n",
            "      There are 2 candidate implementations:\n",
            "            - Of which 2 did not match due to:\n",
            "            Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 159.\n",
            "              With argument(s): '(list(float64)<iv=None>)':\n",
            "             Rejected as the implementation raised a specific error:\n",
            "               TypingError: Failed in nopython mode pipeline (step: nopython frontend)\n",
            "             Unknown attribute 'size' of type list(float64)<iv=None>\n",
            "             \n",
            "             File \"../../../../usr/local/lib/python3.10/dist-packages/numba/np/arraymath.py\", line 666:\n",
            "             def array_argmin_impl_float(arry):\n",
            "                 if arry.size == 0:\n",
            "                 ^\n",
            "             \n",
            "             During: typing of get attribute at /usr/local/lib/python3.10/dist-packages/numba/np/arraymath.py (666)\n",
            "             \n",
            "             File \"../../../../usr/local/lib/python3.10/dist-packages/numba/np/arraymath.py\", line 666:\n",
            "             def array_argmin_impl_float(arry):\n",
            "                 if arry.size == 0:\n",
            "                 ^\n",
            "      \n",
            "        raised from /usr/local/lib/python3.10/dist-packages/numba/core/typeinfer.py:1086\n",
            "      \n",
            "      During: resolving callee type: Function(<function array_argmin_impl_float at 0x7dcfed618160>)\n",
            "      During: typing of call at /usr/local/lib/python3.10/dist-packages/numba/np/arraymath.py (718)\n",
            "      \n",
            "      \n",
            "      File \"../../../../usr/local/lib/python3.10/dist-packages/numba/np/arraymath.py\", line 718:\n",
            "              def array_argmin_impl(arr, axis=None):\n",
            "                  return flatten_impl(arr)\n",
            "                  ^\n",
            "\n",
            "  raised from /usr/local/lib/python3.10/dist-packages/numba/core/typeinfer.py:1086\n",
            "\n",
            "During: resolving callee type: Function(<function argmin at 0x7dd047953010>)\n",
            "During: typing of call at <ipython-input-5-d01188e81256> (53)\n",
            "\n",
            "\n",
            "File \"<ipython-input-5-d01188e81256>\", line 53:\n",
            "def horizontal_seam(energies, penalty_reduction, bidirectional=False):\n",
            "    <source elided>\n",
            "\n",
            "            previous = previous + np.argmin([top, middle, bottom]) - 1\n",
            "            ^\n",
            "\n",
            "  @numba.jit()\n",
            "INFO:numba.core.transforms:finding looplift candidates\n",
            "<ipython-input-5-d01188e81256>:17: NumbaWarning: \n",
            "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"horizontal_seam\" failed type inference due to: Cannot determine Numba type of <class 'numba.core.dispatcher.LiftedLoop'>\n",
            "\n",
            "File \"<ipython-input-5-d01188e81256>\", line 59:\n",
            "def horizontal_seam(energies, penalty_reduction, bidirectional=False):\n",
            "    <source elided>\n",
            "    if bidirectional:\n",
            "        for i in range(width-1, -1, -1):\n",
            "        ^\n",
            "\n",
            "  @numba.jit()\n",
            "/usr/local/lib/python3.10/dist-packages/numba/core/object_mode_passes.py:151: NumbaWarning: Function \"horizontal_seam\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
            "\n",
            "File \"<ipython-input-5-d01188e81256>\", line 27:\n",
            "def horizontal_seam(energies, penalty_reduction, bidirectional=False):\n",
            "    <source elided>\n",
            "    \"\"\"\n",
            "    height, width = energies.shape[:2]\n",
            "    ^\n",
            "\n",
            "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
            "/usr/local/lib/python3.10/dist-packages/numba/core/object_mode_passes.py:161: NumbaDeprecationWarning: \n",
            "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
            "\n",
            "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
            "\n",
            "File \"<ipython-input-5-d01188e81256>\", line 27:\n",
            "def horizontal_seam(energies, penalty_reduction, bidirectional=False):\n",
            "    <source elided>\n",
            "    \"\"\"\n",
            "    height, width = energies.shape[:2]\n",
            "    ^\n",
            "\n",
            "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n",
            "<ipython-input-5-d01188e81256>:17: NumbaWarning: \n",
            "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"horizontal_seam\" failed type inference due to: non-precise type pyobject\n",
            "During: typing of argument at <ipython-input-5-d01188e81256> (59)\n",
            "\n",
            "File \"<ipython-input-5-d01188e81256>\", line 59:\n",
            "def horizontal_seam(energies, penalty_reduction, bidirectional=False):\n",
            "    <source elided>\n",
            "    if bidirectional:\n",
            "        for i in range(width-1, -1, -1):\n",
            "        ^\n",
            "\n",
            "  @numba.jit()\n",
            "/usr/local/lib/python3.10/dist-packages/numba/core/object_mode_passes.py:151: NumbaWarning: Function \"horizontal_seam\" was compiled in object mode without forceobj=True.\n",
            "\n",
            "File \"<ipython-input-5-d01188e81256>\", line 59:\n",
            "def horizontal_seam(energies, penalty_reduction, bidirectional=False):\n",
            "    <source elided>\n",
            "    if bidirectional:\n",
            "        for i in range(width-1, -1, -1):\n",
            "        ^\n",
            "\n",
            "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
            "/usr/local/lib/python3.10/dist-packages/numba/core/object_mode_passes.py:161: NumbaDeprecationWarning: \n",
            "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
            "\n",
            "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
            "\n",
            "File \"<ipython-input-5-d01188e81256>\", line 59:\n",
            "def horizontal_seam(energies, penalty_reduction, bidirectional=False):\n",
            "    <source elided>\n",
            "    if bidirectional:\n",
            "        for i in range(width-1, -1, -1):\n",
            "        ^\n",
            "\n",
            "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n",
            "INFO:root:finished after: 0.012822866439819336 s\n",
            "<ipython-input-7-6cd16fd0c18a>:33: DeprecationWarning: Delaunay attribute 'vertices' is deprecated in favour of 'simplices' and will be removed in Scipy 1.11.0.\n",
            "  overlay_graph = createTINgraph(np.array(list(set(tuple(p) for p in graph_nodes))))\n",
            "INFO:root:finished after: 0.005934476852416992 s\n",
            "INFO:root:finished after: 0.0029404163360595703 s\n",
            "INFO:root:finished after: 0.0043752193450927734 s\n",
            "INFO:root:finished after: 0.0020606517791748047 s\n",
            "INFO:root:finished after: 0.0018420219421386719 s\n",
            "<ipython-input-7-6cd16fd0c18a>:90: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  cv2.polylines(image, np.array([[[np.int(p[1]), np.int(p[0])] for p in polygon]]), 1, color=(248, 24, 148), thickness=3)\n",
            "INFO:root:finished after: 0.023976802825927734 s\n",
            "INFO:root:finished after: 40.80959630012512 s\n",
            "INFO:root:Terminated\n"
          ]
        }
      ],
      "source": [
        "extract_textline(input_path='test1.png', output_path= './output', penalty_reduction = 6000, seam_every_x_pxl= 100,testing=True, vertical=False, console_log= False, small_component_ratio= 0.1)\n",
        "logging.info('Terminated')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dimbi-ej8vXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J17RyQFkxUOg"
      },
      "outputs": [],
      "source": []
    }
  ]
}
